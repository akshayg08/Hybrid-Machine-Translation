{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch \n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.ticker as ticker\n",
    "from torch import nn \n",
    "from tqdm import tqdm\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext.datasets import TranslationDataset\n",
    "from torchtext.data import Field, BucketIterator, Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "MAX_LENGTH = 220\n",
    "BATCH_SIZE = 128\n",
    "CLIP = 5\n",
    "EMB_SIZE = 500\n",
    "NUM_LAYERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src, idx):\n",
    "    mask = (src != idx).unsqueeze(2)\n",
    "    return mask\n",
    "\n",
    "class EncoderRNN(nn.Module):    \n",
    "    def __init__(self, input_size, hidden_size, TEXT):\n",
    "        super(EncoderRNN,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, EMB_SIZE).to(device)\n",
    "        self.gru = nn.GRU(EMB_SIZE, hidden_size, bidirectional = True, batch_first = True).to(device)\n",
    "        self.fc = nn.Linear(2*hidden_size, hidden_size).to(device)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size).to(device)\n",
    "\n",
    "    def forward(self, x, src_len):\n",
    "        embedded = self.embedding(x)\n",
    "        #in order to avoid <pad>\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths = src_len, batch_first = True)\n",
    "        packed_output, hidden = self.gru(packed_embedded)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first = True)\n",
    "        output = self.fc(output) \n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        output = self.layer_norm(output)\n",
    "        hidden = self.layer_norm(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, TEXT):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, EMB_SIZE).to(device)\n",
    "        self.gru = nn.GRU(hidden_size + EMB_SIZE, hidden_size, batch_first = True).to(device)\n",
    "        self.attn_scores1 = nn.Linear(hidden_size + MAX_LENGTH, hidden_size, bias = True).to(device)\n",
    "        self.attn_scores2 = nn.Linear(hidden_size, 1, bias = False).to(device)\n",
    "        self.fc = nn.Linear(hidden_size, output_size).to(device)\n",
    "        self.softmax = nn.Softmax(dim = 1).to(device)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim = 1).to(device)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size).to(device)\n",
    "\n",
    "    def forward(self, x, hidden, encoder_hidden_states, coverage, mask):\n",
    "        embedded = self.embedding(x)\n",
    "        coverage_vec = torch.cat((hidden, coverage), dim = 1)\n",
    "\n",
    "        scores = self.attn_scores2(torch.tanh(torch.add(self.attn_scores1(coverage_vec).unsqueeze(1), encoder_hidden_states)))\n",
    "        attn_weights = scores.masked_fill(mask == 0, -1e10)\n",
    "        attn_weights = self.softmax(attn_weights)\n",
    "\n",
    "        coverage[:, :attn_weights.shape[1]] += attn_weights.squeeze(2)\n",
    "\n",
    "        context_vec = torch.matmul(attn_weights.permute(0, 2, 1), encoder_hidden_states)\n",
    "        attn_hidden = torch.cat((torch.relu(embedded.unsqueeze(dim = 1)), context_vec), dim = 2)\n",
    "\n",
    "        output, hidden = self.gru(attn_hidden, hidden.unsqueeze(dim = 0))\n",
    "        output = self.layer_norm(output)\n",
    "        output = self.fc(output)\n",
    "        output = self.logsoftmax(output.squeeze(dim = 1))\n",
    "        return output, self.layer_norm(hidden.squeeze(0)), attn_weights, coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, src_len, trg):\n",
    "        outputs, hidden = self.encoder(src, src_len)\n",
    "        mask = create_mask(src, SRC_PAD_IDX)\n",
    "        coverage = torch.zeros(src.shape[0], MAX_LENGTH).to(device)\n",
    "        decoder_input = trg[:, 0]\n",
    "        decoder_outputs = torch.zeros(trg.shape[0], output_size, trg.shape[1]).to(device)\n",
    "\n",
    "        for k in range(1, trg.shape[1]):\n",
    "            output, hidden, _, coverage = self.decoder(decoder_input, hidden, outputs, coverage, mask)\n",
    "            decoder_input = trg[:, k]\n",
    "            decoder_outputs[:, :, k] = output\n",
    "\n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(init_token = '<SOS>', eos_token = '<EOS>', lower = True, sequential = True, pad_token = \"<PAD>\", batch_first = True, include_lengths = True)\n",
    "TRG = Field(init_token = '<SOS>', eos_token = '<EOS>', lower = True, sequential = True, pad_token = \"<PAD>\", batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = TranslationDataset.splits(path= \"../data/\", train = \"train\", validation = \"dev\", test = None, exts = (\".hi\", \".en\"), fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter = BucketIterator.splits((train_data, valid_data), batch_size=BATCH_SIZE, sort_key=lambda x: len(x.src), shuffle=True, sort_within_batch = True, device = device)\n",
    "\n",
    "input_size = len(SRC.vocab)\n",
    "output_size = len(TRG.vocab)\n",
    "hidden_size = 500\n",
    "\n",
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "criterion = nn.NLLLoss(ignore_index = TRG_PAD_IDX)\n",
    "\n",
    "encoder = EncoderRNN(input_size, hidden_size, SRC).to(device)\n",
    "decoder = DecoderRNN(hidden_size, output_size, TRG).to(device)\n",
    "model = Model(encoder, decoder).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 3)\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterator, model, optimizer, clip, epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    loop = tqdm(iterator)\n",
    "    for _, batch in enumerate(loop):\n",
    "        src, src_len = batch.src\n",
    "        trg = batch.trg\n",
    "        optimizer.zero_grad()\n",
    "        decoder_outputs = model(src, src_len, trg)\n",
    "        loss = criterion(decoder_outputs[:, :, 1:], trg[:, 1:])\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loop.set_description('Epoch {}/{}'.format(epoch + 1, epochs))\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "    return epoch_loss/len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(iterator, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        for _, batch in enumerate(iterator):\n",
    "            src, src_len = batch.src\n",
    "            trg = batch.trg\n",
    "            decoder_outputs = model(src, src_len, trg)\n",
    "            loss = criterion(decoder_outputs[:, :, 1:], trg[:, 1:])\n",
    "            loss += loss.item()\n",
    "        return loss/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, model, replace_with_src, phrase_table):\n",
    "    model.eval()\n",
    "    tokens = sentence.lower().strip().split()\n",
    "\n",
    "    tokens = [SRC.init_token] + tokens + [SRC.eos_token]\n",
    "    src_indices = [SRC.vocab.stoi[token.strip()] for token in tokens]\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)\n",
    "    src_len = torch.LongTensor([len(src_indices)]).to(device)\n",
    "\n",
    "    coverage = torch.zeros(1, MAX_LENGTH).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs, hidden = model.encoder(src_tensor, src_len)\n",
    "\n",
    "        mask = create_mask(src_tensor, SRC_PAD_IDX)\n",
    "        trg_indices = [TRG.vocab.stoi[TRG.init_token]]\n",
    "\n",
    "        attns = torch.zeros(MAX_LENGTH, len(src_indices)).to(device)\n",
    "        replace_words = []\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_input = torch.LongTensor([trg_indices[-1]]).to(device)\n",
    "            output,  hidden, attn, coverage = model.decoder(decoder_input, hidden, outputs, coverage,mask)\n",
    "            attns[i, :] = attn.squeeze(0).squeeze(1)\n",
    "            pred_token = output.argmax(dim = 1).item()\n",
    "            if pred_token == TRG.vocab.stoi[TRG.unk_token]:\n",
    "                if replace_with_src:\n",
    "                    idx = attn.argmax(axis = 1).item()\n",
    "                    src_token = SRC.vocab.itos[src_indices[idx]]\n",
    "                    if src_token in phrase_table:\n",
    "                        src_token = phrase_table[src_token][0][-1]\n",
    "                    replace_words.append((i, src_token))\n",
    "                    \n",
    "            trg_indices.append(pred_token)\n",
    "            if pred_token == TRG.vocab.stoi[TRG.eos_token]:\n",
    "                break\n",
    "\n",
    "        trg_tokens = [TRG.vocab.itos[j] for j in trg_indices]\n",
    "\n",
    "    return trg_tokens[1:], attns, replace_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentences(epoch, model, replace_with_src, phrase_table):\n",
    "    f1 = open(\"/home/akshay.goindani/NLA/project/models/unk_result\"+str(epoch)+\".txt\", \"w\")\n",
    "    f2 = open(\"/home/akshay.goindani/NLA/project/models/phrase_table_result\"+str(epoch)+\".txt\", \"w\")\n",
    "    with open(\"/home/akshay.goindani/NLA/project/data/test.hi\") as f:\n",
    "        for line in f:\n",
    "            translation, _, replace_words = translate(line.strip(), model, replace_with_src, phrase_table)\n",
    "            f1.write(\" \".join(translation[:-1]).strip() + \"\\n\")\n",
    "            for i in replace_words:\n",
    "                translation[i[0]] = i[1].strip()\n",
    "            f2.write(\" \".join(translation[:-1]).strip() + \"\\n\")\n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    \n",
    "    \n",
    "phrase_table = pickle.load(open(\"../data/threshold_0.0_phrase-table.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 363/363 [01:52<00:00,  2.81it/s, loss=4.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.8819980109033505\n",
      "Validation Loss: 0.8256821036338806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 363/363 [01:59<00:00,  2.64it/s, loss=3.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.243202854779141\n",
      "Validation Loss: 0.7586845755577087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 363/363 [01:57<00:00,  2.88it/s, loss=3.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.356930606621356\n",
      "Validation Loss: 0.7348169088363647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 363/363 [01:56<00:00,  2.30it/s, loss=2.93] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.763206873550888\n",
      "Validation Loss: 0.7334088683128357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 363/363 [01:56<00:00,  3.47it/s, loss=0.784]\n",
      "  0%|          | 0/363 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3721112557186568\n",
      "Validation Loss: 0.7413994669914246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 363/363 [02:00<00:00,  3.38it/s, loss=2.08] \n",
      "  0%|          | 0/363 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0941263524297185\n",
      "Validation Loss: 0.7737724781036377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 363/363 [01:57<00:00,  3.14it/s, loss=0.544]\n",
      "  0%|          | 0/363 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9011978786182141\n",
      "Validation Loss: 0.7824774980545044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 363/363 [01:58<00:00,  2.93it/s, loss=0.556]\n",
      "  0%|          | 0/363 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7238291568294701\n",
      "Validation Loss: 0.8128000497817993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 363/363 [01:58<00:00,  2.74it/s, loss=0.936] \n",
      "  0%|          | 0/363 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4483032576616638\n",
      "Validation Loss: 0.8157902956008911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 363/363 [01:55<00:00,  3.48it/s, loss=0.0543]\n",
      "  0%|          | 0/363 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.32858830058213434\n",
      "Validation Loss: 0.8254267573356628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 363/363 [01:59<00:00,  3.48it/s, loss=0.0474]\n",
      "  0%|          | 0/363 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.27195435809612933\n",
      "Validation Loss: 0.8382109999656677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 363/363 [01:55<00:00,  3.70it/s, loss=0.0882]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.23139938787036363\n",
      "Validation Loss: 0.8515743017196655\n",
      "Epoch 12: early stopping\n"
     ]
    }
   ],
   "source": [
    "best = 1e18\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(train_iter, model, optimizer, CLIP, epoch)\n",
    "    valid_loss = validate(valid_iter, model)\n",
    "    print(\"Train Loss:\", train_loss)\n",
    "    print(\"Validation Loss:\", valid_loss.item())\n",
    "    if valid_loss < best:\n",
    "        best = valid_loss\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"train_loss\": train_loss,\n",
    "            \"valid_loss\": valid_loss\n",
    "        }\n",
    "        translate_sentences(epoch + 1, model, True, phrase_table)\n",
    "        torch.save(checkpoint, \"/home/akshay.goindani/NLA/project/models/model\"+str(epoch+1)+\".tar\")\n",
    "        early_stop = 0\n",
    "    else:\n",
    "        early_stop += 1\n",
    "    scheduler.step(valid_loss)\n",
    "    if early_stop > 7:\n",
    "        print('Epoch %d: early stopping' % (epoch + 1))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"../models/model4.tar\")\n",
    "encoder = EncoderRNN(input_size, hidden_size, SRC).to(device)\n",
    "decoder = DecoderRNN(hidden_size, output_size, TRG).to(device)\n",
    "model = Model(encoder, decoder).to(device)\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1023352\n"
     ]
    }
   ],
   "source": [
    "print(len(phrase_table))\n",
    "translate_sentences(4, model, True, phrase_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
